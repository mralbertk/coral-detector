{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_ImageReframing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install detectron2"
      ],
      "metadata": {
        "id": "zCkc_bRXcLVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==5.1\n",
        "!pip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio===0.10.0+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html \n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
        "\n",
        "import torch\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.\n",
        "\n",
        "# exit(0)  # After installation, you may need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "metadata": {
        "id": "vh0o_Si1YVDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c64cb463-245d-464b-9895-33eac5c1e994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.7/dist-packages (5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
            "Collecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████▌                   | 834.1 MB 96.9 MB/s eta 0:00:14tcmalloc: large alloc 1147494400 bytes == 0x38e94000 @  0x7fb8dfa9d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |███████████████▉                | 1055.7 MB 71.7 MB/s eta 0:00:16tcmalloc: large alloc 1434370048 bytes == 0x7d4ea000 @  0x7fb8dfa9d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████            | 1336.2 MB 85.0 MB/s eta 0:00:10tcmalloc: large alloc 1792966656 bytes == 0x231c000 @  0x7fb8dfa9d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |█████████████████████████▎      | 1691.1 MB 91.8 MB/s eta 0:00:05tcmalloc: large alloc 2241208320 bytes == 0x6d104000 @  0x7fb8dfa9d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0xf2a66000 @  0x7fb8dfa9c1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
            "tcmalloc: large alloc 2672058368 bytes == 0x1e658a000 @  0x7fb8dfa9d615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 396 bytes/s \n",
            "\u001b[?25hCollecting torchvision==0.11.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.5 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio===0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchaudio-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (1.21.6)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.12.0+cu113\n",
            "    Uninstalling torchvision-0.12.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.12.0+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.11.0+cu113\n",
            "    Uninstalling torchaudio-0.11.0+cu113:\n",
            "      Successfully uninstalled torchaudio-0.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0+cu111 torchaudio-0.10.0+cu111 torchvision-0.11.1+cu111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/index.html\n",
            "Collecting detectron2\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.10/detectron2-0.6%2Bcu111-cp37-cp37m-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 578 kB/s \n",
            "\u001b[?25hCollecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.64.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.8.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Collecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Collecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (2022.6.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (4.2.0)\n",
            "Collecting toml>=0.10.1\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (21.3)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.46.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.1.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.11.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.2.0)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=a5cb809c16598b175f8de8990ca9753b77b0014c73bba64fed4c8cd4deea6408\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=3089a694401c4499c7952fb377736510baabe96063b1cdde532636ec8d044717\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, toml, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-21.4b2 detectron2-0.6+cu111 fvcore-0.1.5.post20220512 hydra-core-1.2.0 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.2.2 pathspec-0.9.0 portalocker-2.4.0 toml-0.10.2 typed-ast-1.5.4 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  1.11 ; cuda:  cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "RwdTbX8icO3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "import copy\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "import imutils"
      ],
      "metadata": {
        "id": "q_CkGh55Y2Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All the functions we need"
      ],
      "metadata": {
        "id": "zJxtFPfBfw4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_config(model_path = None, BATCH_SIZE_PER_IMAGE = 512, BASE_LR = 0.00025, MAX_ITER = 1000):\n",
        "\n",
        "  cfg = get_cfg()\n",
        "\n",
        "  # get configuration from model_zoo\n",
        "  config_file = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "  cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
        "\n",
        "  # initialize weights\n",
        "  if not model_path:\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file) \n",
        "  else:\n",
        "    assert os.path.isfile(model_path), '.pth file not found'\n",
        "    cfg.MODEL.WEIGHTS = model_path\n",
        "\n",
        "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (quadrat). \n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set a custom testing threshold\n",
        "  \n",
        "  return cfg"
      ],
      "metadata": {
        "id": "-Xww3dOtYraV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_contour(mask):\n",
        "  # find the contours of the mask\n",
        "  cnts = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "  cnts = imutils.grab_contours(cnts)\n",
        "  # keep only the biggest one\n",
        "  cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:1]\n",
        "  cnts = cnts[0]\n",
        "\n",
        "  assert cnts.shape[1:] == (1,2), \"cnts.shape[1:] should be (1,2)\"\n",
        "\n",
        "  return cnts"
      ],
      "metadata": {
        "id": "n9IZMxH-bYlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minimum_area_enclosing(cnts):\n",
        "  '''\n",
        "  Find the minimum-area rotated rectangle that encloses the given contour\n",
        "  '''\n",
        "  rect = cv2.minAreaRect(cnts)\n",
        "  box = cv2.boxPoints(rect)\n",
        "  box = np.int0(box)\n",
        "  screenCnt = np.array([[i] for i in box.tolist()])\n",
        "  #print('Estimated corners:', screenCnt.tolist())\n",
        "  return screenCnt"
      ],
      "metadata": {
        "id": "AjVlYEOobPgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def line(p1, p2):\n",
        "  '''\n",
        "  Produces coefs A, B, C of line equation by two points provided\n",
        "  Ex: L1 = line([2,3], [4,0]) --> L1 = (3, 2, 12)\n",
        "  '''\n",
        "  A = (p1[1] - p2[1])\n",
        "  B = (p2[0] - p1[0])\n",
        "  C = (p1[0]*p2[1] - p2[0]*p1[1])\n",
        "  return A, B, -C\n",
        "\n",
        "def intersection(L1, L2):\n",
        "  '''\n",
        "  Finds intersection point (if any) of two lines provided by coefs.\n",
        "  Ex: L1 = line([0,1], [2,3])\n",
        "      L2 = line([2,3], [4,0])\n",
        "      intersec = intersection(L1,L2) --> intersec = [2, 3]\n",
        "\n",
        "      L1 = line([0,1], [2,3])\n",
        "      L2 = line([0,2], [2,4])\n",
        "      intersect = intersection(L1,L2) --> None\n",
        "  '''\n",
        "  D  = L1[0] * L2[1] - L1[1] * L2[0]\n",
        "  Dx = L1[2] * L2[1] - L1[1] * L2[2]\n",
        "  Dy = L1[0] * L2[2] - L1[2] * L2[0]\n",
        "\n",
        "  if D != 0:\n",
        "    x = Dx / D\n",
        "    y = Dy / D\n",
        "    return [int(x), int(y)]\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "ZVbwNYVtbLt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lines_intersections(pts, ncols = 1e158, nrows = 1e158):\n",
        "  '''\n",
        "  Input: A list of 4 consecutive lines\n",
        "         Ex: [[[1,2], [3,4]], # line 1 passes through point [1,2] and [3,4]\n",
        "              [[5,6], [7,8]],\n",
        "              [[9,10], [11,12]],\n",
        "              [[13,14], [15,16]]]\n",
        "\n",
        "  Compute: The intersection between each consecutive lines\n",
        "           If the intersection falls outside the image, it is projected on the image\n",
        "\n",
        "  Return: A new contour with the 4 line intersections/the 4 new corners\n",
        "          Ex: array([[[3, 0]],\n",
        "                    [[3, 3]],\n",
        "                    [[0, 3]],\n",
        "                    [[0, 0]]], dtype=int32)\n",
        "\n",
        "  '''\n",
        "  C = [0, 1, 2, 3, 0] \n",
        "  newCnt = []\n",
        "\n",
        "  assert len(pts) == 4, 'There are more than 4 points in the input list'\n",
        "\n",
        "  for corner in range(4):\n",
        "\n",
        "    # First line\n",
        "    p1 = pts[C[corner]]\n",
        "    L1 = line(p1[0], p1[1])\n",
        "\n",
        "    # Second line \n",
        "    p2 = pts[C[corner+1]]\n",
        "    L2 = line(p2[0], p2[1])\n",
        "    \n",
        "    # Find intersection\n",
        "    inter = intersection(L1, L2)\n",
        "    assert inter, 'could not find any intersection between 2 lines'\n",
        "\n",
        "    # Projection on the image\n",
        "    inter = [max(0,min(ncols, inter[0])), max(0,min(nrows, inter[1]))]\n",
        "\n",
        "    newCnt.append([inter])\n",
        "\n",
        "  newCnt = np.array(newCnt, dtype = \"int32\")\n",
        "  #print('New estimated corners:', newCnt.tolist())\n",
        "\n",
        "  return newCnt"
      ],
      "metadata": {
        "id": "S5AUm3HzbJ9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_lines_on_edges(cnts, screenCnt, rows, cols, threshold = 100, epsilon = 100):\n",
        "  '''\n",
        "  Input:\n",
        "  - cnts: the full contour \n",
        "  - screenCnt: 4 estimated corners of the contour, obtained with methods 1 or 2\n",
        "\n",
        "  Steps:\n",
        "  - For each side, delimited by 2 consecutive corners of screenCnt (c1 and c2)\n",
        "    - Find all the points of cnts that are located between c1 and c2 (+- margin)\n",
        "    - Fit a line, using cv2.fitLine, on these points \n",
        "\n",
        "  Output:\n",
        "  - pts: the list of the 4 fitted lines\n",
        "         Ex: [[[1,2], [3,4]], # line 1 passes through point [1,2] and [3,4]\n",
        "              [[5,6], [7,8]],\n",
        "              [[9,10], [11,12]],\n",
        "              [[13,14], [15,16]]]\n",
        "  '''\n",
        "\n",
        "  assert screenCnt.shape == (4, 1, 2), 'screenCnt.shape should be (4, 1, 2)'\n",
        "\n",
        "  X = [i[0][0] for i in cnts] \n",
        "  Y = [i[0][1] for i in cnts] \n",
        "\n",
        "  lefty = [] \n",
        "  righty = []\n",
        "  leftx = []\n",
        "  rightx = []\n",
        "  cutCnt = []\n",
        "  C = [0, 1, 2, 3, 0]\n",
        "\n",
        "  for edge in range(4):\n",
        "    # define 2 extreme points of the edge\n",
        "    c1 = screenCnt[C[edge]][0]\n",
        "    c2 = screenCnt[C[edge+1]][0]\n",
        "\n",
        "    # select all the contour points that are located between these 2 points +- margin --> cutCnt\n",
        "    if abs(c1[0]-c2[0]) < threshold: # if the 2 pts have almost the same abscissa\n",
        "      c_mean = (c1[0] + c2[0])/2\n",
        "      X_margin = [max(0, c_mean - epsilon), min(c_mean + epsilon, cols)]\n",
        "      Y_margin = sorted([c1[1], c2[1]])\n",
        "\n",
        "    elif abs(c1[1]-c2[1]) < threshold: # if the 2 pts have almost the same ordinate\n",
        "      c_mean = (c1[1] + c2[1])/2\n",
        "      X_margin = sorted([c1[0], c2[0]])\n",
        "      Y_margin = [max(0,c_mean - epsilon), min(c_mean + epsilon, rows)] \n",
        "\n",
        "    else:\n",
        "      X_margin = sorted([c1[0], c2[0]])\n",
        "      Y_margin = sorted([c1[1], c2[1]])\n",
        "\n",
        "    c = [[[X[ind], Y[ind]]] for ind in range(len(X)) if X_margin[0] <= X[ind] <= X_margin[1] and Y_margin[0] <= Y[ind] <= Y_margin[1]]\n",
        "    cutCnt.append(np.array(c))\n",
        "\n",
        "    # fit a line on c\n",
        "    [vx, vy, x, y] = cv2.fitLine(np.array(c), cv2.DIST_L2, 0, 0.01, 0.01)\n",
        "\n",
        "    # append the result for this edge to left and right points\n",
        "    leftx.append(0)\n",
        "    rightx.append(cols)\n",
        "    lefty.append(int((-x*vy/vx) + y))\n",
        "    righty.append(int(((cols-x)*vy/vx)+y))\n",
        "\n",
        "  # \n",
        "  pts = [[[leftx[ind], lefty[ind]], [rightx[ind], righty[ind]]] for ind in range(4)]\n",
        "  return pts"
      ],
      "metadata": {
        "id": "EbjyodvTbHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_perspective(img, pts, newSize = 500):\n",
        "  '''\n",
        "  example pts = [[1170,200], [3760,100], [3890,2700], [1250,2800]]\n",
        "  coordinates are ordered such that the first entry in the list is the top-left,\n",
        "\tthe second entry is the top-right, the third is the bottom-right, \n",
        "  and the fourth is the bottom-left\n",
        "  '''\n",
        "\n",
        "  # compute the perspective transform matrix\n",
        "  dst = np.array([\n",
        "\t\t[0, 0],\n",
        "    [0, newSize],\n",
        "    [newSize, newSize],\n",
        "\t\t[newSize, 0]], dtype = \"float32\")\n",
        "  \n",
        "  M = cv2.getPerspectiveTransform(pts, dst)\n",
        "\n",
        "  # apply the transformation matrix \n",
        "  warped = cv2.warpPerspective(img, M, (newSize, newSize))\n",
        "\n",
        "  # return\n",
        "  return warped"
      ],
      "metadata": {
        "id": "3M1hanw-bVdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(img_path, model_path):\n",
        "  \n",
        "  cfg = custom_config(model_path=model_path)\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "\n",
        "  # read image\n",
        "  img = cv2.imread(img_path)\n",
        "  rows, cols = img.shape[:2]\n",
        "\n",
        "  # prediction\n",
        "  output = predictor(img) \n",
        "\n",
        "  # get mask\n",
        "  mask_array = 1*output[\"instances\"].get('pred_masks').to('cpu').numpy()\n",
        "  mask_array = np.moveaxis(mask_array, 0, -1)\n",
        "  mask_array = np.repeat(mask_array, 3, axis=2)\n",
        "  mask3d = np.where(mask_array==False, 0, \n",
        "          (np.where(mask_array==True, 255, img)))\n",
        "  \n",
        "  #mask3d = Image.fromarray(mask3d)\n",
        "  mask = mask3d[:,:,1]\n",
        "\n",
        "  # Find contour of the mask\n",
        "  cnts = find_contour(mask)\n",
        "\n",
        "  ## Fist step : First corners estimations\n",
        "  screenCnt = minimum_area_enclosing(cnts)\n",
        "\n",
        "  ## Second step: Fit 4 lines on each edges\n",
        "  lines_pts = fit_lines_on_edges(cnts, screenCnt, rows, cols)\n",
        "  \n",
        "  ## Third step: Lines intersection to get corners\n",
        "  newCnt = get_lines_intersections(lines_pts, ncols = cols, nrows = rows)\n",
        "\n",
        "  ## Last step: Reframing\n",
        "  warped = transform_perspective(img, pts = np.float32(newCnt[:,0]), newSize = 400)\n",
        "\n",
        "  return warped"
      ],
      "metadata": {
        "id": "yPaUKqgqZadT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "NYK84d78cIDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVUU0imGY8Oo",
        "outputId": "848bee24-71c1-44c2-bd6a-9a52b599e878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_dicts(img_dir):\n",
        "    dataset_dicts = []\n",
        "    img_dir = Path(img_dir)\n",
        "    for i, img_path in enumerate([*img_dir.glob(\"*.jpg\")]):\n",
        "        dataset_dicts.append(str(img_path))\n",
        "    \n",
        "    return dataset_dicts"
      ],
      "metadata": {
        "id": "pcwOFEZQHo66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of testing images\n",
        "imgs = get_dataset_dicts(\"/content/drive/MyDrive/DSTI/coral_reef/img\")"
      ],
      "metadata": {
        "id": "LiWxpSv-H2Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The path of the saved final model\n",
        "model_path_1 = \"/content/drive/MyDrive/DSTI/coral_reef/mlruns/0/e23af3d36ced4c0095619b32eee9e622/artifacts/model_final.pth\"\n",
        "model_path_2 = \"/content/drive/MyDrive/DSTI/coral_reef/mlruns/0/7bd7d8b521ae4ca0a3c964a874f5d768/artifacts/model_final.pth\"\n",
        "model_path_3 = \"/content/drive/MyDrive/DSTI/coral_reef/mlruns/0/02c3f409fa08455d9f58ee6364c86865/artifacts/model_final.pth\" "
      ],
      "metadata": {
        "id": "fAYmoNSxgpeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reframing\n",
        "N = len(imgs)\n",
        "for img_path in random.sample(imgs, 3):\n",
        "  print(img_path)\n",
        "  img_reframed_1 = main(img_path=img_path, model_path=model_path_1)\n",
        "  img_reframed_2 = main(img_path=img_path, model_path=model_path_2)\n",
        "  img_reframed_3 = main(img_path=img_path, model_path=model_path_3)\n",
        "\n",
        "  cv2_imshow(cv2.hconcat([img_reframed_1, img_reframed_2, img_reframed_3]))\n",
        "\n",
        "#cv2_imshow(img_reframed)"
      ],
      "metadata": {
        "id": "c6Rot3DecbVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}